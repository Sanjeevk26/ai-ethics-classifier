{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "f59386f1-56a0-482c-9488-61c6bc5fd5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Mini ML Demo: \"AI Ethics Classifier\"\n",
    "# ============================================================\n",
    "# Goal:\n",
    "# 1) Predict AI ethics principle:\n",
    "#       - Privacy, Transparency, Accountability, Fairness\n",
    "# 2) Predict lifecycle stage:\n",
    "#       - Data, Development, Deployment\n",
    "#\n",
    "# This script is intentionally:\n",
    "# - small, interpretable, expandable\n",
    "# - interactive (quiz + testing)\n",
    "#\n",
    "# Dependencies:\n",
    "#   pip install scikit-learn\n",
    "# ============================================================\n",
    "\n",
    "# =========================\n",
    "# SECTION 1 — IMPORTS + DATA\n",
    "# =========================\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1A) Seed labeled training dataset\n",
    "# ------------------------------------------------------------\n",
    "# We create 50 examples per ethics principle (200 total).\n",
    "# Each example also has a lifecycle label: Data / Development / Deployment.\n",
    "#\n",
    "# NOTE:\n",
    "# - These are synthetic but realistic policy/risk/safeguard statements.\n",
    "# - You can later add your own domain-specific statements to improve accuracy.\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "SeedExample = Tuple[str, str, str]  # (text, ethics_label, lifecycle_label)\n",
    "\n",
    "seed_examples: List[SeedExample] = []\n",
    "\n",
    "# --- PRIVACY (50) ---\n",
    "privacy_data = [\n",
    "    \"Collect only the minimum personal data required for the task.\",\n",
    "    \"Obtain explicit consent before collecting biometric information.\",\n",
    "    \"Use data minimization when building user profiles for personalization.\",\n",
    "    \"Provide clear opt-in choices for collecting location data.\",\n",
    "    \"Remove direct identifiers during preprocessing to reduce re-identification risk.\",\n",
    "    \"Anonymize or pseudonymize records before using them in training datasets.\",\n",
    "    \"Avoid collecting sensitive attributes unless strictly necessary and justified.\",\n",
    "    \"Define a data retention limit and delete records after the purpose is met.\",\n",
    "    \"Use secure transfer when ingesting customer data from third parties.\",\n",
    "    \"Document the lawful basis for collecting personal data in the data pipeline.\",\n",
    "    \"Mask email addresses and phone numbers during dataset preparation.\",\n",
    "    \"Use differential privacy where feasible to protect individual contributions.\",\n",
    "    \"Ensure scraping respects consent signals and site policies to avoid privacy harm.\",\n",
    "    \"Do not combine datasets in ways that could enable identity linkage.\",\n",
    "    \"Strip metadata from uploaded files to prevent accidental data leakage.\",\n",
    "    \"Require signed data processing agreements before sharing user data internally.\",\n",
    "    \"Encrypt data at rest during collection and preprocessing steps.\",\n",
    "]\n",
    "privacy_dev = [\n",
    "    \"Use privacy-preserving training techniques when handling sensitive data.\",\n",
    "    \"Prevent training on raw chat logs unless consent and safeguards are in place.\",\n",
    "    \"Run privacy risk assessments before training on user-generated content.\",\n",
    "    \"Test the model for memorization of personal information during evaluation.\",\n",
    "    \"Block model outputs that could reveal private identifiers from training data.\",\n",
    "    \"Use secure sandbox environments for model development with restricted access.\",\n",
    "    \"Apply access controls so only authorized staff can view sensitive datasets.\",\n",
    "    \"Audit prompts and logs used in fine-tuning to avoid exposing PII.\",\n",
    "    \"Redact sensitive text fields before model training begins.\",\n",
    "    \"Use k-anonymity style checks when preparing training splits.\",\n",
    "    \"Track dataset provenance to ensure personal data was collected appropriately.\",\n",
    "    \"Avoid using shadow datasets that were gathered without user awareness.\",\n",
    "    \"Validate that de-identification survives joins and feature engineering steps.\",\n",
    "    \"Train with federated learning when raw data cannot leave user devices.\",\n",
    "    \"Ensure evaluation datasets do not contain leaked customer identifiers.\",\n",
    "    \"Run membership inference tests to estimate privacy leakage risk.\",\n",
    "]\n",
    "privacy_deploy = [\n",
    "    \"Provide a right-to-delete workflow so users can request data removal.\",\n",
    "    \"Allow users to opt out of data collection without losing core service access.\",\n",
    "    \"Enable users to download and review the personal data the system stores.\",\n",
    "    \"Log access to user data and review logs for suspicious usage patterns.\",\n",
    "    \"Rotate encryption keys and enforce secure storage for deployed logs.\",\n",
    "    \"Avoid storing raw prompts if they may contain personal information.\",\n",
    "    \"Implement automatic redaction of PII in monitoring and analytics.\",\n",
    "    \"Use strict retention policies for inference logs and delete them regularly.\",\n",
    "    \"Offer a privacy notice describing how inference data is used and stored.\",\n",
    "    \"Provide controls to disable personalized tracking in production.\",\n",
    "    \"Ensure incident response includes notification procedures for data breaches.\",\n",
    "    \"Restrict third-party telemetry to prevent unapproved data sharing.\",\n",
    "    \"Use on-device inference for highly sensitive use cases when feasible.\",\n",
    "    \"Provide a privacy contact channel for user requests and complaints.\",\n",
    "    \"Periodically review deployed data flows for scope creep and overcollection.\",\n",
    "    \"Ensure backups also respect deletion requests and retention limits.\",\n",
    "    \"Monitor for prompt injection attempts that try to extract private data.\",\n",
    "]\n",
    "\n",
    "# Make exactly 50 Privacy examples by selecting 17+16+17 = 50\n",
    "privacy_examples = []\n",
    "privacy_examples.extend([(t, \"Privacy\", \"Data\") for t in privacy_data[:17]])\n",
    "privacy_examples.extend([(t, \"Privacy\", \"Development\") for t in privacy_dev[:16]])\n",
    "privacy_examples.extend([(t, \"Privacy\", \"Deployment\") for t in privacy_deploy[:17]])\n",
    "seed_examples.extend(privacy_examples)\n",
    "\n",
    "# --- TRANSPARENCY (50) ---\n",
    "trans_data = [\n",
    "    \"Explain what data is collected and why in plain language.\",\n",
    "    \"Disclose whether user data will be used for training or analytics.\",\n",
    "    \"Provide a data dictionary describing each feature used by the model.\",\n",
    "    \"Clearly label sensitive data fields and how they are processed.\",\n",
    "    \"Publish the data sources used to build the training dataset.\",\n",
    "    \"Document how missing values are handled during preprocessing.\",\n",
    "    \"State the retention period and the purpose for each collected data type.\",\n",
    "    \"Provide users with a clear consent screen rather than hidden defaults.\",\n",
    "    \"Disclose when data is obtained from brokers or third-party providers.\",\n",
    "    \"Explain how data is sampled so users understand representativeness limits.\",\n",
    "    \"Describe what data is excluded and why, including filtering rules.\",\n",
    "    \"Disclose whether synthetic data is used and how it is generated.\",\n",
    "    \"Provide a notice about web scraping and the scope of collected content.\",\n",
    "    \"Explain the impact of normalization, tokenization, and cleaning steps.\",\n",
    "    \"Maintain a changelog for dataset versions used in the project.\",\n",
    "    \"Describe how labels were created and the risk of labeling bias.\",\n",
    "    \"Publish known data limitations and gaps that affect performance.\",\n",
    "]\n",
    "trans_dev = [\n",
    "    \"Use model cards to document intended use and limitations.\",\n",
    "    \"Provide system documentation describing architecture and training setup.\",\n",
    "    \"Explain key evaluation metrics and why they were selected.\",\n",
    "    \"Disclose the main factors influencing high-impact predictions.\",\n",
    "    \"Provide interpretable explanations for model decisions where feasible.\",\n",
    "    \"Document hyperparameters and training configuration for reproducibility.\",\n",
    "    \"Report subgroup performance results to reveal uneven outcomes.\",\n",
    "    \"Explain why a particular algorithm was chosen over alternatives.\",\n",
    "    \"Provide calibration plots or reliability summaries for risk scores.\",\n",
    "    \"Describe the data split method and how leakage was prevented.\",\n",
    "    \"Write clear release notes for each model version.\",\n",
    "    \"Document known failure modes discovered during testing.\",\n",
    "    \"Provide transparency on human review involvement in evaluation.\",\n",
    "    \"Explain the role of heuristics, rules, or post-processing layers.\",\n",
    "    \"Publish a summary of red-teaming findings and mitigations applied.\",\n",
    "    \"Disclose when performance degrades outside the training distribution.\",\n",
    "]\n",
    "trans_deploy = [\n",
    "    \"Provide explanations for high-impact decisions like loans or hiring.\",\n",
    "    \"Tell users when they are interacting with an AI system.\",\n",
    "    \"Disclose confidence or uncertainty signals for model outputs.\",\n",
    "    \"Provide an appeal process when the system makes consequential decisions.\",\n",
    "    \"Explain what monitoring is performed and what triggers interventions.\",\n",
    "    \"Clearly state if user interactions are logged for improvement purposes.\",\n",
    "    \"Provide a user-facing help page describing how the AI works at a high level.\",\n",
    "    \"Display reasons codes for automated approvals or denials when appropriate.\",\n",
    "    \"Disclose when a human is in the loop versus fully automated decisions.\",\n",
    "    \"Publish an uptime and incident report for critical AI services.\",\n",
    "    \"Provide a feedback button so users can report incorrect outputs.\",\n",
    "    \"Notify users when the model version changes in meaningful ways.\",\n",
    "    \"Explain content moderation actions with a clear policy reference.\",\n",
    "    \"Provide transparency reports about enforcement and error rates.\",\n",
    "    \"Explain why certain content was generated or refused by the model.\",\n",
    "    \"Disclose how ranking or recommendation criteria are determined.\",\n",
    "    \"Provide an audit-friendly log of decisions for regulators and users.\",\n",
    "]\n",
    "\n",
    "# 17 + 16 + 17 = 50\n",
    "trans_examples = []\n",
    "trans_examples.extend([(t, \"Transparency\", \"Data\") for t in trans_data[:17]])\n",
    "trans_examples.extend([(t, \"Transparency\", \"Development\") for t in trans_dev[:16]])\n",
    "trans_examples.extend([(t, \"Transparency\", \"Deployment\") for t in trans_deploy[:17]])\n",
    "seed_examples.extend(trans_examples)\n",
    "\n",
    "# --- ACCOUNTABILITY (50) ---\n",
    "acct_data = [\n",
    "    \"Assign an owner responsible for approving data sources and usage rights.\",\n",
    "    \"Require governance review before ingesting new third-party datasets.\",\n",
    "    \"Maintain audit trails for data access and dataset modifications.\",\n",
    "    \"Document who approved collection of sensitive attributes and why.\",\n",
    "    \"Use approval workflows for adding new data fields to production pipelines.\",\n",
    "    \"Ensure data vendors provide compliance documentation and attestations.\",\n",
    "    \"Record dataset lineage so issues can be traced back to origin.\",\n",
    "    \"Define escalation paths when data quality issues are detected.\",\n",
    "    \"Use access reviews to ensure only authorized staff can pull data exports.\",\n",
    "    \"Log data transformations so errors can be reproduced and fixed.\",\n",
    "    \"Assign responsibility for data retention and deletion policy enforcement.\",\n",
    "    \"Require sign-off when combining datasets that increase risk.\",\n",
    "    \"Create accountability checklists for data collection and consent.\",\n",
    "    \"Establish clear roles for data stewards and data custodians.\",\n",
    "    \"Perform periodic audits of dataset compliance with internal policies.\",\n",
    "    \"Document legal review outcomes for data acquisition.\",\n",
    "    \"Require incident reporting when data misuse is suspected.\",\n",
    "]\n",
    "acct_dev = [\n",
    "    \"Assign a named model owner and keep audit trails for training runs.\",\n",
    "    \"Use go/no-go release gates and require documented approvals.\",\n",
    "    \"Run red teaming before release and record mitigation decisions.\",\n",
    "    \"Track evaluation results and who signed off on deployment readiness.\",\n",
    "    \"Maintain reproducible training pipelines so issues can be investigated.\",\n",
    "    \"Require peer review for model changes that affect high-impact outcomes.\",\n",
    "    \"Use checklists for risk assessment before promoting a model version.\",\n",
    "    \"Define responsibility for model performance targets and safety metrics.\",\n",
    "    \"Keep a record of training data versions and code commit hashes.\",\n",
    "    \"Document who can override safeguards and under what conditions.\",\n",
    "    \"Ensure a security review is completed before model launch.\",\n",
    "    \"Set up accountability for responding to discovered bias or harm.\",\n",
    "    \"Require internal audits for compliance with model governance policy.\",\n",
    "    \"Log experiments and rationale for major architectural decisions.\",\n",
    "    \"Establish model review boards for sensitive use cases.\",\n",
    "    \"Ensure evaluation includes adversarial testing and is documented.\",\n",
    "]\n",
    "acct_deploy = [\n",
    "    \"Have incident response playbooks for AI harm and escalation procedures.\",\n",
    "    \"Provide channels for users to report harm and track resolution timelines.\",\n",
    "    \"Monitor the system in production and assign on-call responsibility.\",\n",
    "    \"Define rollback procedures when deployed performance degrades.\",\n",
    "    \"Keep logs sufficient for audits while respecting privacy constraints.\",\n",
    "    \"Investigate complaints and document corrective actions taken.\",\n",
    "    \"Perform post-incident reviews after failures and update safeguards.\",\n",
    "    \"Ensure there is a clear appeals process with accountable decision makers.\",\n",
    "    \"Define SLAs for responding to high-severity model issues.\",\n",
    "    \"Use monitoring dashboards with ownership for each key metric.\",\n",
    "    \"Document how human reviewers are trained and held accountable.\",\n",
    "    \"Record decisions made during crisis management for later audit.\",\n",
    "    \"Establish responsibility for vendor-managed AI components in production.\",\n",
    "    \"Require periodic compliance reviews for deployed AI systems.\",\n",
    "    \"Ensure leadership sign-off for continuing operation after major incidents.\",\n",
    "    \"Maintain a public-facing contact for regulatory inquiries when required.\",\n",
    "    \"Track and report recurring failure patterns and mitigation effectiveness.\",\n",
    "]\n",
    "\n",
    "# 17 + 16 + 17 = 50\n",
    "acct_examples = []\n",
    "acct_examples.extend([(t, \"Accountability\", \"Data\") for t in acct_data[:17]])\n",
    "acct_examples.extend([(t, \"Accountability\", \"Development\") for t in acct_dev[:16]])\n",
    "acct_examples.extend([(t, \"Accountability\", \"Deployment\") for t in acct_deploy[:17]])\n",
    "seed_examples.extend(acct_examples)\n",
    "\n",
    "# --- FAIRNESS (50) ---\n",
    "fair_data = [\n",
    "    \"Ensure representative data coverage across demographic groups.\",\n",
    "    \"Check that data collection does not systematically exclude minorities.\",\n",
    "    \"Balance the dataset to avoid underrepresentation of protected groups.\",\n",
    "    \"Audit labels for bias introduced by annotator assumptions.\",\n",
    "    \"Measure sampling bias when data comes from a single region or platform.\",\n",
    "    \"Avoid proxies for protected attributes unless justified and controlled.\",\n",
    "    \"Document demographic distribution and gaps in the training data.\",\n",
    "    \"Collect data from diverse sources to reduce skewed representation.\",\n",
    "    \"Validate that translation or transcription errors do not target certain accents.\",\n",
    "    \"Ensure accessibility data includes users with disabilities.\",\n",
    "    \"Review whether historical data encodes discriminatory outcomes.\",\n",
    "    \"Avoid using biased crime data without accounting for policing disparities.\",\n",
    "    \"Ensure images include diverse skin tones and lighting conditions.\",\n",
    "    \"Check that data cleaning does not remove dialects disproportionately.\",\n",
    "    \"Assess whether consent processes exclude low-literacy populations.\",\n",
    "    \"Use stratified sampling so minority classes are not ignored.\",\n",
    "    \"Evaluate label consistency across annotators for sensitive categories.\",\n",
    "]\n",
    "fair_dev = [\n",
    "    \"Evaluate performance by subgroup, not just overall accuracy.\",\n",
    "    \"Use fairness metrics like equalized odds or demographic parity where relevant.\",\n",
    "    \"Test for disparate impact in model outputs across protected groups.\",\n",
    "    \"Run bias audits on embeddings and representation learning components.\",\n",
    "    \"Perform counterfactual fairness checks where feasible.\",\n",
    "    \"Use reweighting or resampling to reduce training bias.\",\n",
    "    \"Validate that thresholds do not unfairly penalize one group.\",\n",
    "    \"Check calibration across groups, not only average calibration.\",\n",
    "    \"Include fairness constraints when optimizing high-impact models.\",\n",
    "    \"Use diverse evaluation sets to detect fairness problems early.\",\n",
    "    \"Perform error analysis specifically on historically marginalized groups.\",\n",
    "    \"Document trade-offs between accuracy and fairness metrics.\",\n",
    "    \"Avoid using a single global threshold if it harms certain subgroups.\",\n",
    "    \"Assess whether human review introduces biased overrides.\",\n",
    "    \"Test robustness for different dialects, accents, and writing styles.\",\n",
    "    \"Validate that the model does not amplify stereotypes in generation.\",\n",
    "]\n",
    "fair_deploy = [\n",
    "    \"Monitor fairness drift over time after deployment.\",\n",
    "    \"Track subgroup outcomes and alert when disparities increase.\",\n",
    "    \"Provide a mechanism for impacted users to contest unfair decisions.\",\n",
    "    \"Recalibrate or retrain when group performance gaps widen.\",\n",
    "    \"Run periodic fairness audits on deployed models and publish summaries.\",\n",
    "    \"Use human oversight for high-risk decisions affecting vulnerable groups.\",\n",
    "    \"Monitor feedback for signals of discriminatory behavior in production.\",\n",
    "    \"Ensure content moderation does not silence certain communities unfairly.\",\n",
    "    \"Evaluate recommendation exposure so minority creators are not suppressed.\",\n",
    "    \"Conduct A/B tests with fairness constraints to prevent harmful rollouts.\",\n",
    "    \"Provide accessible support channels for users experiencing discrimination.\",\n",
    "    \"Check post-deployment thresholds and policy rules for disparate impact.\",\n",
    "    \"Use drift detection to identify distribution shifts affecting subgroups.\",\n",
    "    \"Maintain fairness KPIs and assign owners for remediation actions.\",\n",
    "    \"Ensure appeals are reviewed consistently across user groups.\",\n",
    "    \"Audit vendor components for fairness before integrating into production.\",\n",
    "    \"Monitor language model toxicity differentially across identity mentions.\",\n",
    "]\n",
    "\n",
    "# 17 + 16 + 17 = 50\n",
    "fair_examples = []\n",
    "fair_examples.extend([(t, \"Fairness\", \"Data\") for t in fair_data[:17]])\n",
    "fair_examples.extend([(t, \"Fairness\", \"Development\") for t in fair_dev[:16]])\n",
    "fair_examples.extend([(t, \"Fairness\", \"Deployment\") for t in fair_deploy[:17]])\n",
    "seed_examples.extend(fair_examples)\n",
    "\n",
    "# Safety check\n",
    "assert len(seed_examples) == 200, f\"Expected 200 examples, got {len(seed_examples)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2076763e-c0cc-47a6-a894-ce30339e01f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# SECTION 2 — INTERACTIVE QUIZ\n",
    "# ==============================\n",
    "# Purpose:\n",
    "# - Before training, help the user understand what each principle means.\n",
    "# - The quiz is multiple-choice + immediate feedback.\n",
    "#\n",
    "# How it works:\n",
    "# - User answers 12 questions.\n",
    "# - We show score + explanations.\n",
    "# - Then we ask if they want to proceed to training.\n",
    "# ==============================\n",
    "\n",
    "@dataclass\n",
    "class QuizQ:\n",
    "    question: str\n",
    "    options: List[str]        # A, B, C, D\n",
    "    answer: str              # \"A\"/\"B\"/\"C\"/\"D\"\n",
    "    explanation: str\n",
    "\n",
    "QUIZ: List[QuizQ] = [\n",
    "    QuizQ(\n",
    "        question=\"Which principle is MOST about limiting collection and giving users control over personal data?\",\n",
    "        options=[\"A) Fairness\", \"B) Privacy\", \"C) Transparency\", \"D) Accountability\"],\n",
    "        answer=\"B\",\n",
    "        explanation=\"Privacy focuses on consent, minimization, deletion, and preventing exposure of personal data.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"A model denies a loan. The user asks: 'Why?' Which principle is MOST relevant?\",\n",
    "        options=[\"A) Transparency\", \"B) Privacy\", \"C) Fairness\", \"D) Accountability\"],\n",
    "        answer=\"A\",\n",
    "        explanation=\"Transparency includes explainability and meaningful reasons for high-impact decisions.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Who is responsible if the deployed model causes harm and must respond with fixes and documentation?\",\n",
    "        options=[\"A) Transparency\", \"B) Accountability\", \"C) Privacy\", \"D) Fairness\"],\n",
    "        answer=\"B\",\n",
    "        explanation=\"Accountability is about ownership, audit trails, incident response, and corrective action.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"The model performs well overall but poorly for one demographic group. Which principle is violated?\",\n",
    "        options=[\"A) Accountability\", \"B) Privacy\", \"C) Fairness\", \"D) Transparency\"],\n",
    "        answer=\"C\",\n",
    "        explanation=\"Fairness addresses subgroup performance, disparate impact, and equitable outcomes.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Which lifecycle stage MOST relates to consent screens, collection limits, and preprocessing steps?\",\n",
    "        options=[\"A) Deployment\", \"B) Development\", \"C) Data\", \"D) Monitoring\"],\n",
    "        answer=\"C\",\n",
    "        explanation=\"Data stage includes collection + preprocessing (cleaning, de-identification, retention).\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Which lifecycle stage MOST relates to training, hyperparameters, and evaluation?\",\n",
    "        options=[\"A) Development\", \"B) Data\", \"C) Deployment\", \"D) Governance\"],\n",
    "        answer=\"A\",\n",
    "        explanation=\"Development includes training + evaluation, and model documentation like model cards.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Which lifecycle stage MOST relates to monitoring drift, incident response, and user appeals?\",\n",
    "        options=[\"A) Data\", \"B) Development\", \"C) Deployment\", \"D) Pretraining\"],\n",
    "        answer=\"C\",\n",
    "        explanation=\"Deployment includes rollout, monitoring, user feedback, and ongoing risk management.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Publishing a model card describing limitations and intended use is mainly about:\",\n",
    "        options=[\"A) Privacy\", \"B) Transparency\", \"C) Fairness\", \"D) Accountability\"],\n",
    "        answer=\"B\",\n",
    "        explanation=\"Model cards are a transparency practice: clear documentation and disclosure.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Running red-teaming and requiring go/no-go approval before release is mainly about:\",\n",
    "        options=[\"A) Accountability\", \"B) Privacy\", \"C) Transparency\", \"D) Fairness\"],\n",
    "        answer=\"A\",\n",
    "        explanation=\"Accountability includes governance, gates, audits, and responsibility for safe releases.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Removing personal identifiers from datasets is mainly about:\",\n",
    "        options=[\"A) Transparency\", \"B) Privacy\", \"C) Accountability\", \"D) Fairness\"],\n",
    "        answer=\"B\",\n",
    "        explanation=\"De-identification is a core privacy safeguard to reduce exposure and re-identification risk.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Tracking subgroup outcomes after deployment to detect disparities is mainly about:\",\n",
    "        options=[\"A) Fairness\", \"B) Transparency\", \"C) Privacy\", \"D) Accountability\"],\n",
    "        answer=\"A\",\n",
    "        explanation=\"This is fairness monitoring—detecting and correcting unequal outcomes over time.\"\n",
    "    ),\n",
    "    QuizQ(\n",
    "        question=\"Keeping audit trails of training runs and approvals is mainly about:\",\n",
    "        options=[\"A) Transparency\", \"B) Fairness\", \"C) Accountability\", \"D) Privacy\"],\n",
    "        answer=\"C\",\n",
    "        explanation=\"Audit trails and ownership are accountability practices for traceability and remediation.\"\n",
    "    ),\n",
    "]\n",
    "\n",
    "def run_quiz():\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"QUIZ: AI Ethics Principles (12 questions)\")\n",
    "    print(\"Answer by typing A, B, C, or D and pressing Enter.\")\n",
    "    print(\"============================================================\\n\")\n",
    "\n",
    "    score = 0\n",
    "    for i, q in enumerate(QUIZ, 1):\n",
    "        print(f\"Q{i}. {q.question}\")\n",
    "        for opt in q.options:\n",
    "            print(\"   \" + opt)\n",
    "\n",
    "        while True:\n",
    "            ans = input(\"Your answer (A/B/C/D): \").strip().upper()\n",
    "            if ans in {\"A\", \"B\", \"C\", \"D\"}:\n",
    "                break\n",
    "            print(\"Please type only A, B, C, or D.\")\n",
    "\n",
    "        if ans == q.answer:\n",
    "            score += 1\n",
    "            print(\"✅ Correct!\")\n",
    "        else:\n",
    "            print(f\"❌ Not quite. Correct answer: {q.answer}\")\n",
    "        print(\"Explanation:\", q.explanation)\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "    print(f\"\\nFinal Score: {score}/{len(QUIZ)}\")\n",
    "    if score < 9:\n",
    "        print(\"Tip: Review the explanations above—then continue to training to see how labels map to text.\")\n",
    "    else:\n",
    "        print(\"Nice — you’ve got the core concepts. Training will be more intuitive now.\")\n",
    "\n",
    "    while True:\n",
    "        proceed = input(\"\\nProceed to model training? (yes/no): \").strip().lower()\n",
    "        if proceed in {\"yes\", \"no\"}:\n",
    "            return proceed == \"yes\"\n",
    "        print(\"Please type 'yes' or 'no'.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "af101de6-7657-482d-a6b0-31eb0ef76331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# SECTION 3 — TRAINING (WITH CLEAR COMMENTS)\n",
    "# ==========================================\n",
    "# We train a simple, effective baseline:\n",
    "# - TF-IDF turns text into numeric features\n",
    "# - LinearSVC is strong for text classification\n",
    "# - MultiOutputClassifier predicts BOTH labels:\n",
    "#       ethics principle + lifecycle stage\n",
    "#\n",
    "# We also:\n",
    "# - split into train/test\n",
    "# - print metrics for both tasks\n",
    "# ==========================================\n",
    "\n",
    "def train_model(examples: List[SeedExample],\n",
    "                test_size: float = 0.25,\n",
    "                random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Trains a text classifier that predicts:\n",
    "      1) AI ethics principle\n",
    "      2) AI lifecycle stage\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    examples : list of (text, ethics_label, lifecycle_label)\n",
    "        The labeled training data.\n",
    "\n",
    "    test_size : float\n",
    "        Fraction of data to hold out for testing.\n",
    "        0.25 = 25% test, 75% training.\n",
    "\n",
    "    random_state : int\n",
    "        Seed for randomness so results are reproducible.\n",
    "    \"\"\"\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 1: SHUFFLE THE DATA\n",
    "    # ------------------------------------------------------------\n",
    "    # Why?\n",
    "    # ----\n",
    "    # Your dataset was constructed in blocks:\n",
    "    #   - Privacy examples first\n",
    "    #   - Then Transparency\n",
    "    #   - Then Accountability\n",
    "    #   - Then Fairness\n",
    "    #\n",
    "    # If we DON'T shuffle, the train/test split could accidentally\n",
    "    # take mostly one class into the test set or training set.\n",
    "    #\n",
    "    # Shuffling ensures examples are randomly mixed before splitting.\n",
    "    random.Random(random_state).shuffle(examples)\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 2: SEPARATE INPUTS (X) AND OUTPUTS (y)\n",
    "    # ------------------------------------------------------------\n",
    "    # In supervised learning:\n",
    "    #   X = inputs (features)  -> the text statements\n",
    "    #   y = outputs (labels)   -> what we want to predict\n",
    "    #\n",
    "    # Each example is:\n",
    "    #   (text, ethics_label, lifecycle_label)\n",
    "    #\n",
    "    # We split them into separate lists.\n",
    "\n",
    "    # X: the raw text statements\n",
    "    texts = [t for (t, _, _) in examples]\n",
    "\n",
    "    # y_ethics: labels like \"Privacy\", \"Fairness\", etc.\n",
    "    y_ethics = [e for (_, e, _) in examples]\n",
    "\n",
    "    # y_stage: labels like \"Data\", \"Development\", \"Deployment\"\n",
    "    y_stage = [s for (_, _, s) in examples]\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 3: CREATE A MULTI-OUTPUT TARGET\n",
    "    # ------------------------------------------------------------\n",
    "    # We want the model to predict TWO things at once:\n",
    "    #   1) Ethics principle\n",
    "    #   2) Lifecycle stage\n",
    "    #\n",
    "    # scikit-learn expects one \"y\" object, so we combine both labels\n",
    "    # into a single structure:\n",
    "    #\n",
    "    #   Y[i] = (ethics_label_i, lifecycle_label_i)\n",
    "    #\n",
    "    # Example:\n",
    "    #   (\"Privacy\", \"Deployment\")\n",
    "    Y = list(zip(y_ethics, y_stage))\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 4: TRAIN / TEST SPLIT\n",
    "    # ------------------------------------------------------------\n",
    "    # Why split?\n",
    "    # ----------\n",
    "    # We want to measure how well the model performs on\n",
    "    # *unseen* data, not just memorized training data.\n",
    "    #\n",
    "    # train_test_split:\n",
    "    #   - X_train, Y_train → used to TRAIN the model\n",
    "    #   - X_test,  Y_test  → used to EVALUATE the model\n",
    "    #\n",
    "    # stratify=y_ethics:\n",
    "    # ------------------\n",
    "    # This ensures each ethics principle appears in roughly\n",
    "    # the same proportion in both train and test sets.\n",
    "    #\n",
    "    # Without stratification, the test set might accidentally\n",
    "    # contain very few examples of one principle.\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "        texts,\n",
    "        Y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y_ethics\n",
    "    )\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 5: DEFINE THE CLASSIFIER\n",
    "    # ------------------------------------------------------------\n",
    "    # LinearSVC (Support Vector Classifier):\n",
    "    # ------------------------------------\n",
    "    # - Very strong baseline for text classification\n",
    "    # - Works well with high-dimensional sparse data (TF-IDF)\n",
    "    # - Fast and reliable for small/medium datasets\n",
    "    #\n",
    "    # IMPORTANT:\n",
    "    # ----------\n",
    "    # LinearSVC does NOT output probabilities by default.\n",
    "    # It outputs class predictions directly.\n",
    "    base_clf = LinearSVC()\n",
    "\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # STEP 6: BUILD THE PIPELINE\n",
    "    # ------------------------------------------------------------\n",
    "    # A Pipeline chains multiple steps together so that:\n",
    "    #   - During training: each step is fitted in order\n",
    "    #   - During prediction: the same transformations are applied\n",
    "    #\n",
    "    # Our pipeline has TWO stages:\n",
    "    #\n",
    "    # (1) TF-IDF Vectorizer\n",
    "    # --------------------\n",
    "    # Converts raw text into numerical features.\n",
    "    #\n",
    "    # - TF (Term Frequency):\n",
    "    #     How often a word appears in a document\n",
    "    #\n",
    "    # - IDF (Inverse Document Frequency):\n",
    "    #     Downweights words that appear in many documents\n",
    "    #\n",
    "    # ngram_range=(1, 2):\n",
    "    # -------------------\n",
    "    # - Use single words (unigrams): \"delete\"\n",
    "    # - AND two-word phrases (bigrams): \"right delete\"\n",
    "    #\n",
    "    # This helps capture short phrases common in policy language.\n",
    "    #\n",
    "    # min_df=1:\n",
    "    # ---------\n",
    "    # - Keep even rare words (since dataset is small)\n",
    "    #\n",
    "    # (2) MultiOutputClassifier\n",
    "    # -------------------------\n",
    "    # Wraps a classifier so it can predict MULTIPLE labels.\n",
    "    #\n",
    "    # Internally:\n",
    "    # - One LinearSVC is trained for ethics principle\n",
    "    # - One LinearSVC is trained for lifecycle stage\n",
    "    #\n",
    "    # Same text → two predictions.\n",
    "    model = Pipeline([\n",
    "        (\"tfidf\", TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            min_df=1\n",
    "        )),\n",
    "        (\"clf\", MultiOutputClassifier(base_clf))\n",
    "    ])\n",
    "\n",
    "    # ------------------------------------------------------------\n",
    "    # The model is now READY to be trained using:\n",
    "    #     model.fit(X_train, Y_train)\n",
    "    #\n",
    "    # And evaluated using:\n",
    "    #     model.predict(X_test)\n",
    "    #\n",
    "    # (Those steps happen outside this snippet.)\n",
    "    # ------------------------------------------------------------\n",
    "\n",
    "    # ----------------------------\n",
    "    # TRAINING STEP\n",
    "    # ----------------------------\n",
    "    # Fit the pipeline on training data:\n",
    "    # - Vectorizer learns vocabulary\n",
    "    # - Classifier learns patterns between words/phrases and labels\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    # ----------------------------\n",
    "    # EVALUATION STEP\n",
    "    # ----------------------------\n",
    "    Y_pred = model.predict(X_test)\n",
    "\n",
    "    # Separate predictions for reporting\n",
    "    ethics_true = [y[0] for y in Y_test]\n",
    "    stage_true = [y[1] for y in Y_test]\n",
    "    ethics_pred = [y[0] for y in Y_pred]\n",
    "    stage_pred = [y[1] for y in Y_pred]\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"EVALUATION RESULTS (holdout test set)\")\n",
    "    print(\"============================================================\\n\")\n",
    "\n",
    "    print(\"---- Ethics Principle Classification Report ----\")\n",
    "    print(classification_report(ethics_true, ethics_pred, digits=3))\n",
    "\n",
    "    print(\"---- Lifecycle Stage Classification Report ----\")\n",
    "    print(classification_report(stage_true, stage_pred, digits=3))\n",
    "\n",
    "    # Confusion matrices (compact view)\n",
    "    print(\"---- Confusion Matrix: Ethics (rows=true, cols=pred) ----\")\n",
    "    labels_ethics = [\"Privacy\", \"Transparency\", \"Accountability\", \"Fairness\"]\n",
    "    print(labels_ethics)\n",
    "    print(confusion_matrix(ethics_true, ethics_pred, labels=labels_ethics))\n",
    "\n",
    "    print(\"\\n---- Confusion Matrix: Lifecycle (rows=true, cols=pred) ----\")\n",
    "    labels_stage = [\"Data\", \"Development\", \"Deployment\"]\n",
    "    print(labels_stage)\n",
    "    print(confusion_matrix(stage_true, stage_pred, labels=labels_stage))\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae19dfd9-7d75-47dc-9287-199e836e5c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed labeled examples: 200\n",
      "Principles: 50 each (Privacy, Transparency, Accountability, Fairness)\n",
      "\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS (holdout test set)\n",
      "============================================================\n",
      "\n",
      "---- Ethics Principle Classification Report ----\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "Accountability      0.714     0.769     0.741        13\n",
      "      Fairness      0.857     0.500     0.632        12\n",
      "       Privacy      0.714     0.769     0.741        13\n",
      "  Transparency      0.800     1.000     0.889        12\n",
      "\n",
      "      accuracy                          0.760        50\n",
      "     macro avg      0.771     0.760     0.750        50\n",
      "  weighted avg      0.769     0.760     0.750        50\n",
      "\n",
      "---- Lifecycle Stage Classification Report ----\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Data      0.667     0.842     0.744        19\n",
      "  Deployment      0.688     0.579     0.629        19\n",
      " Development      0.500     0.417     0.455        12\n",
      "\n",
      "    accuracy                          0.640        50\n",
      "   macro avg      0.618     0.613     0.609        50\n",
      "weighted avg      0.635     0.640     0.631        50\n",
      "\n",
      "---- Confusion Matrix: Ethics (rows=true, cols=pred) ----\n",
      "['Privacy', 'Transparency', 'Accountability', 'Fairness']\n",
      "[[10  0  2  1]\n",
      " [ 0 12  0  0]\n",
      " [ 2  1 10  0]\n",
      " [ 2  2  2  6]]\n",
      "\n",
      "---- Confusion Matrix: Lifecycle (rows=true, cols=pred) ----\n",
      "['Data', 'Development', 'Deployment']\n",
      "[[16  2  1]\n",
      " [ 3  5  4]\n",
      " [ 5  3 11]]\n",
      "\n",
      "============================================================\n",
      "BUILT-IN TEST CASES\n",
      "============================================================\n",
      "\n",
      "Text: We should allow users to delete their data and stop collecting analytics logs.\n",
      "Predicted -> Principle: Privacy | Lifecycle: Deployment\n",
      "\n",
      "Text: Provide a clear explanation when the system rejects a job application.\n",
      "Predicted -> Principle: Transparency | Lifecycle: Deployment\n",
      "\n",
      "Text: Assign a model owner and create incident response procedures for harmful outputs.\n",
      "Predicted -> Principle: Accountability | Lifecycle: Deployment\n",
      "\n",
      "Text: Evaluate the model across gender and region to reduce disparate impact.\n",
      "Predicted -> Principle: Fairness | Lifecycle: Development\n",
      "\n",
      "Text: Document training data sources and known limitations in a model card.\n",
      "Predicted -> Principle: Transparency | Lifecycle: Development\n",
      "\n",
      "Text: Monitor subgroup performance after launch and retrain when drift is detected.\n",
      "Predicted -> Principle: Accountability | Lifecycle: Deployment\n",
      "\n",
      "Text: Remove identifiers before training and restrict access to sensitive datasets.\n",
      "Predicted -> Principle: Privacy | Lifecycle: Development\n",
      "\n",
      "Text: Disclose when users are interacting with an AI chatbot and why it responded that way.\n",
      "Predicted -> Principle: Transparency | Lifecycle: Deployment\n",
      "\n",
      "============================================================\n",
      "INTERACTIVE TESTING\n",
      "Type a statement and press Enter to classify it.\n",
      "Type 'quit' to stop.\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement (or 'quit'):  Minorities did not get loans\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicted Principle: Fairness\n",
      "→ Predicted Lifecycle: Data\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement (or 'quit'):  The user was not aware of what was being done with his data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicted Principle: Transparency\n",
      "→ Predicted Lifecycle: Data\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement (or 'quit'):  Personally Identifiable Information should be removed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Predicted Principle: Privacy\n",
      "→ Predicted Lifecycle: Data\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a statement (or 'quit'):  quit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Done. You can now expand seed_examples with your own labeled data for higher accuracy.\n"
     ]
    }
   ],
   "source": [
    "# ======================================\n",
    "# SECTION 4 — TESTING / TRY YOUR OWN TEXT\n",
    "# ======================================\n",
    "# After training:\n",
    "# - Predict on custom statements interactively\n",
    "# - Provide a small set of built-in test cases\n",
    "# ======================================\n",
    "\n",
    "def predict_statement(model, text: str):\n",
    "    pred = model.predict([text])[0]\n",
    "    ethics_label, stage_label = pred[0], pred[1]\n",
    "    return ethics_label, stage_label\n",
    "\n",
    "def run_builtin_tests(model):\n",
    "    tests = [\n",
    "        \"We should allow users to delete their data and stop collecting analytics logs.\",\n",
    "        \"Provide a clear explanation when the system rejects a job application.\",\n",
    "        \"Assign a model owner and create incident response procedures for harmful outputs.\",\n",
    "        \"Evaluate the model across gender and region to reduce disparate impact.\",\n",
    "        \"Document training data sources and known limitations in a model card.\",\n",
    "        \"Monitor subgroup performance after launch and retrain when drift is detected.\",\n",
    "        \"Remove identifiers before training and restrict access to sensitive datasets.\",\n",
    "        \"Disclose when users are interacting with an AI chatbot and why it responded that way.\",\n",
    "    ]\n",
    "\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"BUILT-IN TEST CASES\")\n",
    "    print(\"============================================================\")\n",
    "    for t in tests:\n",
    "        e, s = predict_statement(model, t)\n",
    "        print(f\"\\nText: {t}\")\n",
    "        print(f\"Predicted -> Principle: {e} | Lifecycle: {s}\")\n",
    "\n",
    "def interactive_testing_loop(model):\n",
    "    print(\"\\n============================================================\")\n",
    "    print(\"INTERACTIVE TESTING\")\n",
    "    print(\"Type a statement and press Enter to classify it.\")\n",
    "    print(\"Type 'quit' to stop.\")\n",
    "    print(\"============================================================\\n\")\n",
    "\n",
    "    while True:\n",
    "        text = input(\"Enter a statement (or 'quit'): \").strip()\n",
    "        if text.lower() == \"quit\":\n",
    "            break\n",
    "        if len(text) < 10:\n",
    "            print(\"Please enter a bit more detail (at least ~10 characters).\")\n",
    "            continue\n",
    "\n",
    "        e, s = predict_statement(model, text)\n",
    "        print(f\"→ Predicted Principle: {e}\")\n",
    "        print(f\"→ Predicted Lifecycle: {s}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# =========================\n",
    "# MAIN PROGRAM FLOW\n",
    "# =========================\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Seed labeled examples:\", len(seed_examples))\n",
    "    print(\"Principles: 50 each (Privacy, Transparency, Accountability, Fairness)\\n\")\n",
    "\n",
    "    # 1) Run quiz BEFORE training\n",
    "   # proceed = run_quiz()\n",
    "   # if not proceed:\n",
    "   #     print(\"\\nOkay — stopping here. Re-run when you want to train the model.\")\n",
    "   #     raise SystemExit(0)\n",
    "\n",
    "    # 2) Train + evaluate\n",
    "    model = train_model(seed_examples, test_size=0.25, random_state=42)\n",
    "\n",
    "    # 3) Built-in tests\n",
    "    run_builtin_tests(model)\n",
    "\n",
    "    # 4) User tries their own statements\n",
    "    interactive_testing_loop(model)\n",
    "\n",
    "    print(\"\\nDone. You can now expand seed_examples with your own labeled data for higher accuracy.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923b22fd-7106-40fc-b1bd-835fd6ce5fef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
